import pandas as pd
import numpy as np
from sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit, GridSearchCV
from sklearn.linear_model import LogisticRegression
import warnings

warnings.filterwarnings("ignore")


def split_dataset(df, split_ratio):
    n_train = int(df.shape[0] * split_ratio)

    x_train_data = df.iloc[:n_train, :-2]
    x_test_data = df.iloc[n_train:, :-2]

    y_train_data = df.iloc[:n_train, -2:]
    y_test_data = df.iloc[n_train:, -2:]

    print(x_train_data.shape, x_test_data.shape, y_train_data.shape, y_test_data.shape)

    return x_train_data, x_test_data, y_train_data, y_test_data


def get_class_weight(y_train):
    y_labels = np.reshape(y_train[['sparse_target']].values, (-1))
    max_count = pd.Series(y_labels).value_counts().max()
    return (pd.Series(y_labels).value_counts() / max_count).to_dict()


def main(df, split_ratio=0.80, models=(LogisticRegression)):
    '''
    Search the best model on test set
    :param df: dataset generated by preprocess::main()
    :param split_ratio: part used for training and validation, the rest part of unseen data for testing
    :return: best model and best parameters
    '''

    x_train_data, x_test_data, y_train_data, y_test_data = split_dataset(df, split_ratio)

    class_weight = get_class_weight(y_train_data)

    print(class_weight)

    if (False):
        parameters = {[{'solver': ('liblinear', 'saga'), 'penalty': ['l1', 'l2'], 'C': [0.1, 1, 10]},
                      {'solver': ('newton-cg', 'lbfgs', 'sag'), 'penalty': ['l2'], 'C': [0.1, 1, 10]}]}
        model = LogisticRegression(class_weight=class_weight, random_state=True)
        clf = GridSearchCV(model, parameters, cv=4)
        clf.fit(x_train_data.values, y_train_data[['sparse_target']].values)

        print(clf.best_params_)
        model = clf.best_estimator_
    else:
        best_parameters = {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}
        #best_parameters = {'C': 0.1, 'penalty': 'l2', 'solver': 'saga'}
        model = LogisticRegression(class_weight=class_weight, random_state=True, **best_parameters)
        model.fit(x_train_data.values, y_train_data[['sparse_target']].values)

    y_pred = model.predict(x_test_data)

    y_pred_proba = model.predict_proba(x_test_data)
    y_pred_proba = [y_pred_proba[i, v] for i, v in enumerate(np.argmax(y_pred_proba, axis=1))]
    y_pred_proba = np.array(y_pred_proba)

    y_test_data_sparse = y_test_data[['sparse_target']].values
    y_test_data_sparse = np.reshape(y_test_data_sparse, (-1))
    y_test_data_return = y_test_data[['target']].values
    y_test_data_return = np.reshape(y_test_data_return, (-1))

    #print(y_pred.shape, y_test_data_sparse.shape, y_pred_proba.shape, y_test_data_return.shape)
    result = y_pred * y_test_data_sparse * y_pred_proba * y_test_data_return
    # non zero measurament
    nz = result[np.where(result !=0)]
    nz_coverage = (float(len(nz))/len(result))
    nz_mean = np.mean(nz)
    nz_std = np.std(nz)
    sharpe_ratio = nz_mean/nz_std
    # Precision score
    print(len(np.where(nz > 0)), len(nz))
    nz_prec = np.sum(np.where(nz > 0, 1, 0)) / float(len(nz))

    print('Total >>>>>', np.sum(result))
    print('Prec. >>>>> %.3f%%' % (nz_prec*100))
    print('Cover.>>>>> %.3f%%' % (nz_coverage*100))
    print('Sharp >>>>> %.3f' % sharpe_ratio)
    print('Mean  >>>>> %.3f' % nz_mean)
    print('Std   >>>>> %.3f' % nz_std)
    print('Detail>>>>>', result)
    pass


if __name__ == '__main__':
    df = pd.read_csv('../trasi.csv', index_col='index')
    models = [

    ]
    main(df)
